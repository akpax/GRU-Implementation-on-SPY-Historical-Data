# -*- coding: utf-8 -*-
"""SE207_Final_project_weekly.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-_RO7NBfKZ-jdvpnUdjkrCMvSSBkpPr3

**SE207 Final Project**

Predicting opening Stock Price using GRU

Goal: Compare a GRU on opening stock price on daily time frame versus weekly
"""

import torch, time, copy
import torch.nn as nn
import numpy as np
import sklearn as skl
import matplotlib.pyplot as plt
import shutil
import pandas as pd
from datetime import datetime

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, StandardScaler

from google.colab import drive
drive.mount('/content/drive')

global device, print_freq
print_freq = 50
device = torch.device('cpu')

#read daily $SPY data from .csv file
data_daily = pd.read_csv('/content/drive/MyDrive/SPY_1993_2022.csv')
print(data_daily)

#clean data from .csv file
for i in range(19,6,-1):
  data_daily.drop(data_daily.columns[[i]],axis=1, inplace=True)

#drop adjusted close prices
data_daily.drop(data_daily.columns[[5,6]],axis=1, inplace=True)
print(data_daily)
#drop_vec = ['7', '8', '9', '10', '11', '12', '13', '14', '15', '16', '17', '18', '19']
#for i in range(0,len(drop_vec)-1):
 # data.drop([f'Unnamed: {drop_vec[i]}'], axis = 1)

#read daily $SPY data from .csv file
data_weekly = pd.read_csv('/content/drive/MyDrive/SPY-weekly.csv')
#drop adjusted close 
data_weekly.drop(data_weekly.columns[[5,6]],axis=1, inplace=True)
print(data_weekly)

data_weekly.Date[0]

#convert date strings into date time objects
def convert_to_datetime(date_str):
  format_str = '%m/%d/%y' # The format
  datetime_object= datetime.strptime(date_str, format_str)
  return datetime_object

test = convert_to_datetime('1/1/94')
type(test)

#convert date strings into date time objects ofr daily and weekly time frames
data_weekly.Date = data_weekly.Date.apply(convert_to_datetime)
data_daily.Date = data_daily.Date.apply(convert_to_datetime)

type(data_weekly.Date[0])



import matplotlib.ticker as tick
fig, ax = plt.subplots()
ax.plot(data_daily.Date,data_daily.Open)
def y_fmt(x,y):
    return '${}'.format(x)


ax.yaxis.set_major_formatter(tick.FuncFormatter(y_fmt))
ax.grid()
ax.title.set_text('Price-History of SPY')
ax.set_xlabel('Time')
ax.set_ylabel('Price')

fig, ax = plt.subplots()
ax.plot(data_daily.Date,data_daily.Open,label = 'Daily Period')
ax.plot(data_weekly.Date,data_weekly.Open, label = 'Weekly Period')
def y_fmt(x,y):
    return '${}'.format(x)


ax.yaxis.set_major_formatter(tick.FuncFormatter(y_fmt))
ax.grid()
ax.title.set_text('Price-History of SPY')
ax.set_xlabel('Time')
ax.set_ylabel('Price')
ax.set_xlim(convert_to_datetime('1/1/20'), convert_to_datetime('7/1/20'))
ax.legend(loc = "upper right")





"""##Setup"""

import numpy as np
import pandas as pd
import math
import sklearn
import sklearn.preprocessing
import datetime
import os
import matplotlib.pyplot as plt
import tensorflow as tf

# split data in 80%/10%/10% train/validation/test sets
valid_set_size_percentage = 10 
test_set_size_percentage = 10 

seq_len = 5

gru_weekly_df = data_weekly.copy()
#gru_weekly_df.drop(index = ('Volume'))
gru_weekly_df.head()

weekly_raw = gru_weekly_df.to_numpy()
weekly_raw[0:5]

weekly_data = []
for index in range(len(weekly_raw) - seq_len):
  weekly_data.append(weekly_raw[index:index+seq_len])

weekly_data = np.array(weekly_data)

"""#Maniplate cell"""

#scale data
import sklearn
def normalize_data(df):
  min_max_scaler = sklearn.preprocessing.MinMaxScaler()
  df.Open = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))
  df.High = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))
  df.Low = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))
  df.Close = min_max_scaler.fit_transform(df.Close.values.reshape(-1,1))
 
  return df

def split_data(df,train_size):
  """
  Splits data frame into training, testing, and vlaidation sets.
  Input train_size, test_size, validation_size as decimal)
  Splits data not included in training set into two equal length np.array for testing and validation
  """

  tot_length = len(df)
  train_index = int(round(tot_length*train_size))
  end_val_index = int((tot_length- train_index)/2 +train_index)
  df_train = df[0:train_index+1]
  df_val = df[train_index+1:end_val_index+1]
  df_test = df[end_val_index+1:tot_length+1]
  
  if len(df_train) +len(df_val) + len(df_test) == tot_length:
    print('split successful')
  return [df_train, df_val, df_test]


  


    
#data_norm = data.copy()
#data_norm = normalize_data(data_norm)
#print(data_norm)

#normalize weekly and daily stock data
 data_weekly_norm = normalize_data(data_weekly)
 data_daily_norm = normalize_data(data_daily)
 
 train_size = 0.8
 
 #split up weekly data
 [X_train_weekly, X_val_weekly, Y_train_weekly] = split_data(data_weekly_norm.Open.copy(), train_size)
 #split up daily data
 [X_train_daily, X_val_daily, Y_test_daily] = split_data(data_daily_norm.Open.copy(), train_size)
 print(X_train_weekly.shape)
 print(X_val_weekly.shape)
 print(Y_test_weekly.shape)
 print(X_train_weekly)

plt.figure(figsize=(15, 5));
plt.plot(data_daily_norm.Open, color='yellow', label='open')
plt.plot(data_daily_norm.Close, color='red', label='low')
plt.plot(data_daily_norm.Low, color='blue', label='low')
plt.plot(data_daily_norm.High, color='orange', label='high')
#plt.plot(df_stock_norm.volume.values, color='gray', label='volume')
plt.title('Normalized Price History of SPY')
plt.xlabel('time [days]')
plt.ylabel('normalized price/volume')
plt.legend(loc='best')
plt.show()

## Basic Cell RNN in tensorflow

index_in_epoch = 0;
perm_array  = np.arange(X_train_weekly.shape[0])
#np.random.shuffle(perm_array)

perm_array



X_train_weekly

weekly_data

"""## GRU 

"""

gru_weekly_df.head

#gru_weekly_df.drop(index = ('Volume'), axis=0, inplace=True)
gru_weekly_df.head

gru_weekly_df = gru_weekly_df.set_index('Date')

gru_weekly_df.head

def normalize_data(df):
    min_max_scaler = sklearn.preprocessing.MinMaxScaler()
    df['Open'] = min_max_scaler.fit_transform(df.Open.values.reshape(-1,1))
    df['High'] = min_max_scaler.fit_transform(df.High.values.reshape(-1,1))
    df['Low'] = min_max_scaler.fit_transform(df.Low.values.reshape(-1,1))
    df['Close'] = min_max_scaler.fit_transform(df['Close'].values.reshape(-1,1))
    return df

def norm_data(stock, seq_len):
    data_raw = stock.to_numpy() # convert to numpy array
    data = []
    
    # create all possible sequences of length seq_len
    for index in range(len(data_raw) - seq_len): 
        data.append(data_raw[index: index + seq_len])
    
    data = np.array(data);
    valid_set_size = int(np.round(valid_set_size_percentage/100*data.shape[0]));  
    test_set_size = int(np.round(test_set_size_percentage/100*data.shape[0]));
    train_set_size = data.shape[0] - (valid_set_size + test_set_size);
    
    X_train_weekly = data[:train_set_size,:-1,:]
    Y_train_weekly = data[:train_set_size,-1,:]
    
    X_val_weekly = data[train_set_size:train_set_size+valid_set_size,:-1,:]
    Y_valid_weekly = data[train_set_size:train_set_size+valid_set_size,-1,:]
    
    X_test_weekly = data[train_set_size+valid_set_size:,:-1,:]
    Y_test_weekly = data[train_set_size+valid_set_size:,-1,:]
    
    return [X_train_weekly, Y_train_weekly, X_val_weekly, Y_valid_weekly, X_test_weekly, Y_test_weekly]

gru_weekly_norm = gru_weekly_df.copy()

gru_weekly_norm = normalize_data(gru_weekly_norm)

# create train, test data
seq_len = 25 # choose sequence length
X_train_weekly, Y_train_weekly, X_val_weekly, Y_val_weekly, X_test_weekly, Y_test_weekly = norm_data(gru_weekly_norm, seq_len)
print('X_train_weekly.shape = ',X_train_weekly.shape)
print('Y_train_weekly.shape = ', Y_train_weekly.shape)
print('X_val_weekly.shape = ',X_val_weekly.shape)
print('Y_val_weekly.shape = ', Y_val_weekly.shape)
print('X_test_weekly.shape = ', X_test_weekly.shape)
print('Y_test_weekly.shape = ',Y_test_weekly.shape)
print(X_train_weekly)

print(X_train_weekly.shape)

index_in_epoch = 0
perm_array = np.arange(X_train_weekly.shape[0])
np.random.shuffle(perm_array)

# function to get the next batch
def get_next_batch(batch_size):
    global index_in_epoch, X_train_weekly, perm_array   
    start = index_in_epoch
    index_in_epoch += batch_size

    if index_in_epoch > X_train_weekly.shape[0]:
      np.random.shuffle(perm_array) # shuffle permutation array
      start = 0 # start next epoch
      index_in_epoch = batch_size
        
    end = index_in_epoch
    return X_train_weekly[perm_array[start:end]], Y_train_weekly[perm_array[start:end]]




n_steps = seq_len-1 
n_inputs = 4 
n_neurons = 200 
n_outputs = 4
n_layers = 2
learning_rate = 0.001
batch_size = 30
n_epochs = 100 
train_set_size = X_train_weekly.shape[0]
test_set_size = X_test_weekly.shape[0]

tf.compat.v1.reset_default_graph()
tf.compat.v1.disable_eager_execution()

X = tf.compat.v1.placeholder(tf.float32, [None, n_steps, n_inputs])
y = tf.compat.v1.placeholder(tf.float32, [None, n_outputs])

# use GRU cell
layers = [tf.compat.v1.nn.rnn_cell.GRUCell(num_units=n_neurons, activation=tf.nn.leaky_relu)
          for layer in range(n_layers)]

multi_layer_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(layers)
rnn_outputs, states = tf.compat.v1.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32)

stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, n_neurons])
stacked_outputs = tf.compat.v1.layers.dense(stacked_rnn_outputs, n_outputs)
outputs = tf.reshape(stacked_outputs, [-1, n_steps, n_outputs])
outputs = outputs[:, n_steps - 1, :]  # keep only last output of sequence

loss = tf.reduce_mean(input_tensor=tf.square(outputs - y))  # loss function = mean squared error
optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=learning_rate)
training_op = optimizer.minimize(loss)

# run graph
with tf.compat.v1.Session() as sess:
    sess.run(tf.compat.v1.global_variables_initializer())
    for iteration in range(int(n_epochs * train_set_size / batch_size)):
        
        x_batch, y_batch = get_next_batch(batch_size)  # fetch the next training batch
        
        
        sess.run(training_op, feed_dict={X: x_batch, y: y_batch})
        if iteration % int(5 * train_set_size / batch_size) == 0:
            mse_train = loss.eval(feed_dict={X: X_train_weekly, y: Y_train_weekly})
            mse_valid = loss.eval(feed_dict={X: X_val_weekly, y: Y_val_weekly})
            print('%.2f epochs: MSE train/valid = %.6f/%.6f' % (
                iteration * batch_size / train_set_size, mse_train, mse_valid))

    Y_train_weekly_pred = sess.run(outputs, feed_dict={X: X_train_weekly})
    Y_val_weekly_pred = sess.run(outputs, feed_dict={X: X_val_weekly})
    Y_test_weekly_pred = sess.run(outputs, feed_dict={X: X_test_weekly})

ft = 0 # 0 = open, 1 = close, 2 = highest, 3 = lowest

## show predictions
plt.figure(figsize=(15, 5));
plt.subplot(1,2,1);

plt.plot(np.arange(Y_train_weekly.shape[0]), Y_train_weekly[:,ft], color='blue', label='train target')

plt.plot(np.arange(Y_train_weekly.shape[0], Y_train_weekly.shape[0]+Y_val_weekly.shape[0]), Y_val_weekly[:,ft],
         color='gray', label='valid target')

plt.plot(np.arange(Y_train_weekly.shape[0]+Y_val_weekly.shape[0],
                   Y_train_weekly.shape[0]+Y_test_weekly.shape[0]+Y_test_weekly.shape[0]),
         Y_test_weekly[:,ft], color='black', label='test target')

plt.plot(np.arange(Y_train_weekly_pred.shape[0]),Y_train_weekly_pred[:,ft], color='red',
         label='train prediction')

plt.plot(np.arange(Y_train_weekly_pred.shape[0], Y_train_weekly_pred.shape[0]+Y_val_weekly_pred.shape[0]),
         Y_val_weekly_pred[:,ft], color='orange', label='valid prediction')

plt.plot(np.arange(Y_train_weekly_pred.shape[0]+Y_val_weekly_pred.shape[0],
                   Y_train_weekly_pred.shape[0]+Y_val_weekly_pred.shape[0]+Y_test_weekly_pred.shape[0]),
         Y_test_weekly_pred[:,ft], color='green', label='test prediction')

plt.title('past and future stock prices')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');

plt.subplot(1,2,2);

plt.plot(np.arange(Y_train_weekly.shape[0], Y_train_weekly.shape[0]+Y_test_weekly.shape[0]),
         Y_test_weekly[:,ft], color='black', label='test target')

plt.plot(np.arange(Y_train_weekly_pred.shape[0], Y_train_weekly_pred.shape[0]+Y_test_weekly_pred.shape[0]),
         Y_test_weekly_pred[:,ft], color='green', label='test prediction')

plt.title('future stock prices')
plt.xlabel('time [days]')
plt.ylabel('normalized price')
plt.legend(loc='best');

corr_price_development_train = np.sum(np.equal(np.sign(Y_train_weekly[:,1]-Y_train_weekly[:,0]),
            np.sign(Y_train_weekly_pred[:,1]-Y_train_weekly_pred[:,0])).astype(int)) / Y_train_weekly.shape[0]
corr_price_development_valid = np.sum(np.equal(np.sign(Y_val_weekly[:,1]-Y_val_weekly[:,0]),
            np.sign(Y_val_weekly_pred[:,1]-Y_val_weekly_pred[:,0])).astype(int)) / Y_val_weekly.shape[0]
corr_price_development_test = np.sum(np.equal(np.sign(Y_test_weekly[:,1]-Y_test_weekly[:,0]),
            np.sign(Y_test_weekly_pred[:,1]-Y_test_weekly_pred[:,0])).astype(int)) / Y_test_weekly.shape[0]

print('correct sign prediction for close - open price for train/valid/test: %.2f/%.2f/%.2f'%(
    corr_price_development_train, corr_price_development_valid, corr_price_development_test))



